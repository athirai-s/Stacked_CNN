{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruthi-Gopal/Fake-News-Detection---Stacked-CNN/blob/main/Glove_Embeddings_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YUvgjDMWycc",
        "outputId": "b6d29f42-a917-45b7-8cb8-e88c091834f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "base_path=\"/content/gdrive/MyDrive/fake_news/\"\n",
        "drive.mount('/content/gdrive')\n",
        "df=pd.read_csv(base_path+\"gossipcop_real.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"value\"] = 1\n",
        "df.to_csv(base_path+\"gossipcop_real.csv\", index=False)"
      ],
      "metadata": {
        "id": "Z_QF9vMZW3W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"value\"] = 1\n",
        "df.to_csv(base_path+\"gossipcop_real.csv\", index=False)"
      ],
      "metadata": {
        "id": "-7MANYGvW6vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(base_path+\"gossipcop_fake.csv\")\n",
        "df[\"value\"] = 0\n",
        "df.to_csv(base_path+\"gossipcop_fake.csv\", index=False)"
      ],
      "metadata": {
        "id": "loACBvy_W8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(base_path+\"politifact_fake.csv\")\n",
        "df[\"value\"] = 0\n",
        "df.to_csv(base_path+\"politifact_fake.csv\", index=False)"
      ],
      "metadata": {
        "id": "yPlF6nxXXAcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(base_path+\"politifact_real.csv\")\n",
        "df[\"value\"] = 1\n",
        "df.to_csv(base_path+\"politifact_real.csv\", index=False)"
      ],
      "metadata": {
        "id": "yeXgfH46XB32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the dataset\n",
        "df1 = pd.read_csv(base_path+\"gossipcop_real.csv\", usecols = ['title'])\n",
        "df2 = pd.read_csv(base_path+\"politifact_real.csv\", usecols = ['title'])\n",
        "df3 = pd.read_csv(base_path+\"gossipcop_fake.csv\", usecols = ['title'])\n",
        "df4 = pd.read_csv(base_path+\"politifact_fake.csv\", usecols = ['title'])"
      ],
      "metadata": {
        "id": "fBZdZ5w4XDO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining real and fake data into seperate datasets\n",
        "df_real = pd.concat([df1, df2], axis=0)\n",
        "df_fake = pd.concat([df3, df4], axis=0)"
      ],
      "metadata": {
        "id": "4rDRYoa1XE2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove sentences having less than 5 words\n",
        "df_real = df_real[~df_real.title.str.count('\\s+').lt(4)]"
      ],
      "metadata": {
        "id": "gpc5mH5FXGWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add label column for real news\n",
        "#0 -> real\n",
        "#1 -> fake\n",
        "df_real['label'] = 0\n",
        "df_real"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6KNJNuMuXH8n",
        "outputId": "dc285c20-fc96-4771-e8de-d9db2aaa585e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 title  label\n",
              "0    khloe kardashian tristan thompson scandal ever...      0\n",
              "1        wwe update real life heat alexa bliss nia jax      0\n",
              "2    tina fey jimmy fallon seth meyers return snl w...      0\n",
              "3    meghan markle find prince harry stop jealous k...      0\n",
              "4    victoria beckham james corden carpool karaoke ...      0\n",
              "..                                                 ...    ...\n",
              "618  Trump asking Congress, not Mexico, to pay for ...      0\n",
              "619  Flake: “Religious tests should have no place i...      0\n",
              "620                           Change We Can Believe In      0\n",
              "621  deputy director of national health statistics ...      0\n",
              "622  Romneys ProLife Conversion Myth or Reality Jun...      0\n",
              "\n",
              "[21485 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f05ca2d-ccc9-43e6-a305-583bc5a66ca4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>khloe kardashian tristan thompson scandal ever...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wwe update real life heat alexa bliss nia jax</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tina fey jimmy fallon seth meyers return snl w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>meghan markle find prince harry stop jealous k...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>victoria beckham james corden carpool karaoke ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>Trump asking Congress, not Mexico, to pay for ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>Flake: “Religious tests should have no place i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>620</th>\n",
              "      <td>Change We Can Believe In</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>621</th>\n",
              "      <td>deputy director of national health statistics ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>Romneys ProLife Conversion Myth or Reality Jun...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21485 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f05ca2d-ccc9-43e6-a305-583bc5a66ca4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f05ca2d-ccc9-43e6-a305-583bc5a66ca4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f05ca2d-ccc9-43e6-a305-583bc5a66ca4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove sentences having less than 5 words\n",
        "df_fake = df_fake[~df_fake.title.str.count('\\s+').lt(4)]\n",
        "\n",
        "#Add label column for fake news\n",
        "df_fake['label'] = 1\n",
        "df_fake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "kvoiupU0XMtY",
        "outputId": "8f272720-e70c-44bf-9adc-895b66cd6c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-d53ba246ec16>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_fake['label'] = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 title  label\n",
              "0    Did Miley Cyrus and Liam Hemsworth secretly ge...      1\n",
              "1    Paris Jackson & Cara Delevingne Enjoy Night Ou...      1\n",
              "2    Celebrities Join Tax March in Protest of Donal...      1\n",
              "3    Cindy Crawford's daughter Kaia Gerber wears a ...      1\n",
              "4        Full List of 2018 Oscar Nominations – Variety      1\n",
              "..                                                 ...    ...\n",
              "426  BUSTED: Russian Mansions Obama Seized Were Mea...      1\n",
              "427        Who is affected by the government shutdown?      1\n",
              "428  Lindsey Graham Threatens To Convert To Democra...      1\n",
              "429  ELECTORAL COLLEGE ELECTOR COMMITS SUICIDE TO A...      1\n",
              "430  Sarah Palin Calls To Boycott Mall Of America B...      1\n",
              "\n",
              "[5433 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0feba135-a14d-49ea-8d8b-a43e1e6d7dec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Did Miley Cyrus and Liam Hemsworth secretly ge...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Paris Jackson &amp; Cara Delevingne Enjoy Night Ou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Celebrities Join Tax March in Protest of Donal...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cindy Crawford's daughter Kaia Gerber wears a ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Full List of 2018 Oscar Nominations – Variety</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>BUSTED: Russian Mansions Obama Seized Were Mea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>Who is affected by the government shutdown?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>Lindsey Graham Threatens To Convert To Democra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>ELECTORAL COLLEGE ELECTOR COMMITS SUICIDE TO A...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>Sarah Palin Calls To Boycott Mall Of America B...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5433 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0feba135-a14d-49ea-8d8b-a43e1e6d7dec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0feba135-a14d-49ea-8d8b-a43e1e6d7dec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0feba135-a14d-49ea-8d8b-a43e1e6d7dec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine both the real and fake news dataset\n",
        "df = pd.concat([df_real, df_fake])\n",
        "\n",
        "#Shuffle the final dataset\n",
        "df = df.sample(frac=1)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Iy6uKEWuXNON",
        "outputId": "3541ac5f-67cd-4df1-8144-fd167a0b2c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   title  label\n",
              "3214           brad pitt jen aniston mexico wedding plan      0\n",
              "13291  happens abbey recap kim tell elizabeth done ex...      0\n",
              "20401  report reese witherspoon meryl streep power st...      0\n",
              "18805  george h w bush hospitalized low blood pressur...      0\n",
              "18899            vanderpump rule finale jax taylor quits      0\n",
              "...                                                  ...    ...\n",
              "10521         tv scoop award vote favorite breakout star      0\n",
              "1638   sag award nomination see full list nominee var...      0\n",
              "2254                            big little lie tv series      0\n",
              "1557    riverdale season new release date cast plot hint      0\n",
              "8091        member british royal family ranked net worth      0\n",
              "\n",
              "[26918 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95c61b2b-d580-4bd4-8b6a-012cf96c8fe3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3214</th>\n",
              "      <td>brad pitt jen aniston mexico wedding plan</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13291</th>\n",
              "      <td>happens abbey recap kim tell elizabeth done ex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20401</th>\n",
              "      <td>report reese witherspoon meryl streep power st...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18805</th>\n",
              "      <td>george h w bush hospitalized low blood pressur...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18899</th>\n",
              "      <td>vanderpump rule finale jax taylor quits</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10521</th>\n",
              "      <td>tv scoop award vote favorite breakout star</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1638</th>\n",
              "      <td>sag award nomination see full list nominee var...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2254</th>\n",
              "      <td>big little lie tv series</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1557</th>\n",
              "      <td>riverdale season new release date cast plot hint</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8091</th>\n",
              "      <td>member british royal family ranked net worth</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26918 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95c61b2b-d580-4bd4-8b6a-012cf96c8fe3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95c61b2b-d580-4bd4-8b6a-012cf96c8fe3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95c61b2b-d580-4bd4-8b6a-012cf96c8fe3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the distinct label values\n",
        "df.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBSgLG6KXPKk",
        "outputId": "4b4791f0-111b-45bd-afe2-8e49047db558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    21485\n",
              "1     5433\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "HkB2QmdqXRO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download stop words -> common words that dont add significant meaning to the text\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kW2zzXNXWa6",
        "outputId": "fce9e9e7-57bc-4dd5-fcbf-75a7bdb27ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download WordNet (lexical database for English language)->\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGHzR0rnXYIY",
        "outputId": "b3ef668d-07d4-4985-8839-66321aed0163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uilw8qp_XZuK",
        "outputId": "ff6520d1-a6f8-44a1-f5b1-e54cd8659c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text preprocessing\n",
        "def preprocessing(tweet):\n",
        "  text = BeautifulSoup(tweet).get_text() #Remove HTML tags\n",
        "  text = re.sub(\"[^a-zA-Z#]\", \" \", text) #Remove special characters\n",
        "  text = re.sub('((www.[^s]+)|(https?://[^s]+))',' ', text) #Remove URLs\n",
        "  text = text.lower().split() #Convert to lowercase and split each word\n",
        "\n",
        "  stop_w = set(stopwords.words(\"english\")) #Use a set instead of list for faster searching\n",
        "  text = [w for w in text if not w in stop_w] #Remove stop words\n",
        "  text = [WordNetLemmatizer().lemmatize(w) for w in text] #Lemmatization\n",
        "\n",
        "  return (\" \".join(text)) #Return the words after joining each word separated by space"
      ],
      "metadata": {
        "id": "2wBNBQHSXbHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean the textual data\n",
        "df['title'] = df['title'].fillna({i: [{}] for i in df.index})\n",
        "df['title'] = df['title'].apply(lambda text: preprocessing(str(text)))\n",
        "df['title'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIS9OU_gXdea",
        "outputId": "51890339-9b07-4123-c562-fc193098dcbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3214             brad pitt jen aniston mexico wedding plan\n",
              "13291    happens abbey recap kim tell elizabeth done ex...\n",
              "20401    report reese witherspoon meryl streep power st...\n",
              "18805    george h w bush hospitalized low blood pressur...\n",
              "18899              vanderpump rule finale jax taylor quits\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "\n",
        "from joblib import dump"
      ],
      "metadata": {
        "id": "tVkFSFVBXfv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "train, test = train_test_split(df, test_size = 0.2, stratify = df['label'], random_state = 42)\n",
        "\n",
        "# Get the shape of datasets after splitting\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGLIJnl1Xhr8",
        "outputId": "34a90788-9757-4df4-d6da-bcd978b87216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21534, 2), (5384, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the GloVe word embeddings\n",
        "embedding_dict = {}\n",
        "with open('glove.6B.100d.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_dict[word] = coefs\n"
      ],
      "metadata": {
        "id": "49GAnf1vXvNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['title'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)"
      ],
      "metadata": {
        "id": "B9FQpeRkgUPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences\n",
        "max_len = 100\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_len)"
      ],
      "metadata": {
        "id": "5Mg4RGBtgcIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer.word_index))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  embedding_vector = embedding_dict.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "id": "SnZ1M0iygfUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7643fae-66de-49d1-e5dc-f9d164d0e0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14973\n",
            "(14974, 100)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RrZSwdXmgj6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-QkatGsgqYq",
        "outputId": "42ae96c0-c8d6-485f-e75b-907dc77e9d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "673/673 [==============================] - 19s 25ms/step - loss: 0.4841 - accuracy: 0.7968 - val_loss: 0.4753 - val_accuracy: 0.8013\n",
            "Epoch 2/10\n",
            "673/673 [==============================] - 17s 25ms/step - loss: 0.4598 - accuracy: 0.7944 - val_loss: 0.4739 - val_accuracy: 0.7936\n",
            "Epoch 3/10\n",
            "673/673 [==============================] - 16s 23ms/step - loss: 0.4453 - accuracy: 0.7932 - val_loss: 0.4921 - val_accuracy: 0.7942\n",
            "Epoch 4/10\n",
            "673/673 [==============================] - 16s 23ms/step - loss: 0.4305 - accuracy: 0.7923 - val_loss: 0.5033 - val_accuracy: 0.7775\n",
            "Epoch 5/10\n",
            "673/673 [==============================] - 17s 25ms/step - loss: 0.4176 - accuracy: 0.7921 - val_loss: 0.5292 - val_accuracy: 0.7823\n",
            "Epoch 6/10\n",
            "673/673 [==============================] - 16s 23ms/step - loss: 0.4047 - accuracy: 0.7930 - val_loss: 0.5370 - val_accuracy: 0.7541\n",
            "Epoch 7/10\n",
            "673/673 [==============================] - 15s 23ms/step - loss: 0.3941 - accuracy: 0.7937 - val_loss: 0.5576 - val_accuracy: 0.7405\n",
            "Epoch 8/10\n",
            "673/673 [==============================] - 15s 23ms/step - loss: 0.3853 - accuracy: 0.7920 - val_loss: 0.5837 - val_accuracy: 0.7283\n",
            "Epoch 9/10\n",
            "673/673 [==============================] - 16s 23ms/step - loss: 0.3762 - accuracy: 0.7948 - val_loss: 0.6011 - val_accuracy: 0.7194\n",
            "Epoch 10/10\n",
            "673/673 [==============================] - 15s 23ms/step - loss: 0.3684 - accuracy: 0.7946 - val_loss: 0.6309 - val_accuracy: 0.6855\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faa372fe940>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the CNN model on the testing set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dc-oygzg0NM",
        "outputId": "f5d61ad0-e967-4c79-f7e5-2add852e2c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169/169 [==============================] - 1s 8ms/step - loss: 0.6309 - accuracy: 0.6855\n",
            "Test loss: 0.6309080123901367\n",
            "Test accuracy: 0.685549795627594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_prediction = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIBSiKxkg2Lv",
        "outputId": "6029f02a-22dc-4a31-d57f-74d87fe4d634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169/169 [==============================] - 2s 12ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "base_path=\"/content/gdrive/MyDrive/fake_news/\"\n",
        "df=pd.read_csv(base_path+\"gossipcop_real.csv\")\n",
        "\n",
        "df[\"value\"] = 1\n",
        "df.to_csv(base_path+\"gossipcop_real.csv\", index=False)\n",
        "\n",
        "df=pd.read_csv(base_path+\"gossipcop_fake.csv\")\n",
        "df[\"value\"] = 0\n",
        "df.to_csv(base_path+\"gossipcop_fake.csv\", index=False)\n",
        "\n",
        "df=pd.read_csv(base_path+\"politifact_fake.csv\")\n",
        "df[\"value\"] = 0\n",
        "df.to_csv(base_path+\"politifact_fake.csv\", index=False)\n",
        "\n",
        "df=pd.read_csv(base_path+\"politifact_real.csv\")\n",
        "df[\"value\"] = 1\n",
        "df.to_csv(base_path+\"/politifact_real.csv\", index=False)"
      ],
      "metadata": {
        "id": "AkZ05dOiPNYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the dataset\n",
        "df1 = pd.read_csv(base_path+\"gossipcop_real.csv\", usecols = ['title'])\n",
        "df2 = pd.read_csv(base_path+\"politifact_real.csv\", usecols = ['title'])\n",
        "df3 = pd.read_csv(base_path+\"gossipcop_fake.csv\", usecols = ['title'])\n",
        "df4 = pd.read_csv(base_path+\"politifact_fake.csv\", usecols = ['title'])\n",
        "\n",
        "#combining real and fake data into seperate datasets\n",
        "df_real = pd.concat([df1, df2], axis=0)\n",
        "df_fake = pd.concat([df3, df4], axis=0)\n",
        "\n",
        "# Check for empty and NA values\n",
        "import numpy as np\n",
        "\n",
        "df_real[df_real.loc[:,:] == ' '] = np.NaN\n",
        "print(df_real.isna().sum())\n",
        "df_fake[df_fake.loc[:,:] == ' '] = np.NaN\n",
        "print(df_fake.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezxaNrDAY0cz",
        "outputId": "d4d8066f-0458-476a-e27d-b200177cefe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title    2\n",
            "dtype: int64\n",
            "title    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove sentences having less than 5 words\n",
        "df_real = df_real[~df_real.title.str.count('\\s+').lt(4)]\n",
        "\n",
        "#Add label column for real news\n",
        "#0 -> real\n",
        "#1 -> fake\n",
        "df_real['label'] = 0\n",
        "df_real\n",
        "\n",
        "#Remove sentences having less than 5 words\n",
        "df_fake = df_fake[~df_fake.title.str.count('\\s+').lt(4)]\n",
        "\n",
        "#Add label column for fake news\n",
        "df_fake['label'] = 1\n",
        "df_fake\n",
        "\n",
        "#Combine both the real and fake news dataset\n",
        "df = pd.concat([df_real, df_fake])\n",
        "\n",
        "#Shuffle the final dataset\n",
        "df = df.sample(frac=1)\n",
        "df\n",
        "\n",
        "#Count the distinct label values\n",
        "df.label.value_counts()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NbmssqMZKGb",
        "outputId": "e155e5f1-7070-43bc-b7cf-546575bc0336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-8b37a6c527fe>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_real['label'] = 0\n",
            "<ipython-input-34-8b37a6c527fe>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_fake['label'] = 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    21485\n",
              "1     5433\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text preprocessing\n",
        "def preprocessing(tweet):\n",
        "  text = BeautifulSoup(tweet).get_text() #Remove HTML tags\n",
        "  text = re.sub(\"[^a-zA-Z#]\", \" \", text) #Remove special characters\n",
        "  text = re.sub('((www.[^s]+)|(https?://[^s]+))',' ', text) #Remove URLs\n",
        "  text = text.lower().split() #Convert to lowercase and split each word\n",
        "\n",
        "  stop_w = set(stopwords.words(\"english\")) #Use a set instead of list for faster searching\n",
        "  text = [w for w in text if not w in stop_w] #Remove stop words\n",
        "  text = [WordNetLemmatizer().lemmatize(w) for w in text] #Lemmatization\n",
        "\n",
        "  return (\" \".join(text)) #Return the words after joining each word separated by space"
      ],
      "metadata": {
        "id": "3R3APLRSZ94X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean the textual data\n",
        "df['title'] = df['title'].fillna({i: [{}] for i in df.index})\n",
        "df['title'] = df['title'].apply(lambda text: preprocessing(str(text)))\n",
        "df['title'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEglKgCsaAJZ",
        "outputId": "799eabe8-21c4-496c-cefd-cd596d65ce99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144                          christina el moussa net worth\n",
              "13298    assassination gianni versace review grim portr...\n",
              "16488      prince harry meghan markle want baby right away\n",
              "18800        presley gerber enjoys date cayley king g show\n",
              "4762                                       live kelly ryan\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "\n",
        "from joblib import dump"
      ],
      "metadata": {
        "id": "8RvTdy5-aWpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "train, test = train_test_split(df, test_size = 0.2, stratify = df['label'], random_state = 42)\n",
        "\n",
        "# Get the shape of datasets after splitting\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ-B1w3WaaA1",
        "outputId": "1694e17c-046e-4b1a-9616-9c6cf772df39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21534, 2), (5384, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TF-IDF vectorizer object\n",
        "tfidf_vec = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))\n",
        "\n",
        "# Data fitting and transformation\n",
        "train_df = tfidf_vec.fit_transform(train.title)\n",
        "test_df  = tfidf_vec.transform(test.title)"
      ],
      "metadata": {
        "id": "PgArhuOhacwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SMOTE (Synthetic Minority Oversampling Technique) for dealing with class imbalance\n",
        "smt = SMOTE(random_state = 18, sampling_strategy = 1.0)\n",
        "smt_xtrain_df, smt_ytrain = smt.fit_resample(train_df, train.label)\n",
        "\n",
        "# After over-sampling the minority class\n",
        "smt_ytrain.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01XGHrEiam58",
        "outputId": "d8b77678-9005-414d-9d5f-2c25f988610e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    17188\n",
              "1    17188\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "i41NSPktasxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEwZaAQJa0Fj",
        "outputId": "eb8c5da8-e67a-4b07-9d82-5d716a3d0413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Preprocess the data\n",
        "X = df['title'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Convert text to bag of words\n",
        "vectorizer = CountVectorizer(max_features=10000)\n",
        "X = vectorizer.fit_transform(X).toarray()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "8IMdcZEma3--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base classifer 2 => KNN\n",
        "# Select top K features using the chi-squared test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "k_best = SelectKBest(chi2, k=10000)\n",
        "X = k_best.fit_transform(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Define the KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the KNN model\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the KNN model on the testing set\n",
        "accuracy = knn_model.score(X_test, y_test)\n",
        "\n",
        "#predict on the testing set\n",
        "knn_prediction = knn_model.predict(X_test)\n",
        "\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZO9heB6mZ1y",
        "outputId": "765cdbb6-d8b4-4084-9fc4-f247d1ec731b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7316121842496285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#base classifier => SVM\n",
        "# Define the SVM model\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "svm_model = svm.SVC(kernel='linear')\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "svm_predicatin = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the SVM model on  testing set\n",
        "accuracy = accuracy_score(y_test, svm_predicatin)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "iOPWVYbhTbrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f04c2918-0263-4848-9d29-64b1f3a92d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7035661218424963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#base classifier => decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Train a decision tree classifier\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier\n",
        "decision_tree_prediction = decision_tree.predict(X_test)\n",
        "print(classification_report(y_test, decision_tree_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2o9SNQ8WW2y",
        "outputId": "62d143bf-9c14-4a67-b684-b6ea03024173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.80      0.78      4283\n",
            "           1       0.03      0.03      0.03      1101\n",
            "\n",
            "    accuracy                           0.64      5384\n",
            "   macro avg       0.40      0.41      0.41      5384\n",
            "weighted avg       0.61      0.64      0.63      5384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#base classifier => Naive bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier\n",
        "naive_bayes_prediction = naive_bayes.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, naive_bayes_prediction)\n",
        "print(classification_report(y_test, naive_bayes_prediction))\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt_F2lZ4aE4P",
        "outputId": "cc2301a0-0adb-4296-a8cc-5912c257a80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83      4283\n",
            "           1       0.38      0.45      0.41      1101\n",
            "\n",
            "    accuracy                           0.74      5384\n",
            "   macro avg       0.61      0.63      0.62      5384\n",
            "weighted avg       0.75      0.74      0.74      5384\n",
            "\n",
            "0.736812778603269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) KNN + CNN -> SVM => CKS\n",
        "#SVM as meta level classifier -> input : cnn + knn\n",
        "# Combine the predictions from the base level classifiers\n",
        "base_level_predictions = np.column_stack((cnn_prediction, knn_prediction))\n",
        "\n",
        "# Train the meta level classifier - > SVM classifier\n",
        "cks = svm.SVC(kernel='linear')\n",
        "cks.fit(base_level_predictions, y_test)\n",
        "\n",
        "\n",
        "# Use the meta level classifier to make the final prediction\n",
        "cks_prediction = cks.predict(base_level_predictions)\n",
        "\n",
        "# Calculate the accuracy of the final prediction\n",
        "accuracy = accuracy_score(y_test, cks_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X91-BRBLaM_5",
        "outputId": "aa179345-6688-486c-9511-33df755e0c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) KNN + CNN -> KNN => CKK\n",
        "# Use base level predictions to train meta classifier\n",
        "ckk_meta_train = np.concatenate((cnn_prediction.reshape(-1, 1), knn_prediction.reshape(-1, 1)), axis=1)\n",
        "ckk = KNeighborsClassifier(n_neighbors=5)\n",
        "ckk.fit(ckk_meta_train, y_test)\n",
        "\n",
        "# Generate predictions from base level classifiers on meta level training data\n",
        "cnn_meta_train = model.predict(X_train)\n",
        "knn_meta_train = knn_model.predict(X_train)\n",
        "\n",
        "# Use base level predictions to make predictions using meta classifier\n",
        "ckk_meta_test = np.concatenate((cnn_meta_train.reshape(-1, 1), knn_meta_train.reshape(-1, 1)), axis=1)\n",
        "ckk_prediction = ckk.predict(ckk_meta_test)\n",
        "\n",
        "# Evaluate ensemble classifier\n",
        "accuracy = accuracy_score(y_train, ckk_prediction)\n",
        "print(\"Accuracy of ensemble classifier: {:.2f}%\".format(accuracy*100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmqFo7vGaT2Q",
        "outputId": "134e8d9a-0253-4f87-c104-475ad7b39064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of ensemble classifier: 78.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) KNN + CNN -> NaiveBayes => CKNB\n",
        "# Use base level predictions to train meta classifier\n",
        "base_level_predictions = np.column_stack((cnn_prediction, knn_prediction))\n",
        "cknb = MultinomialNB()\n",
        "cknb.fit(base_level_predictions, y_test)\n",
        "\n",
        "cknb_prediction = cknb.predict(base_level_predictions)\n",
        "\n",
        "# Evaluate ensemble classifier\n",
        "accuracy = accuracy_score(y_test, cknb_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhlW74wfax9N",
        "outputId": "465611fd-4a1e-4676-b43e-c1d8933a42fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) CNN+ KNN→ Decision tree => CKD\n",
        "ckd = DecisionTreeClassifier()\n",
        "ckd.fit(base_level_predictions, y_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "ckd_prediction = ckd.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(y_test, ckd_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TW_3HiZazww",
        "outputId": "93db9e1d-2f29-4a02-ca1a-2c29ee870741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9856983655274889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5) CNN+ Naive bayes classifier → SVM => CNBS\n",
        "# Use base level predictions to train meta classifier\n",
        "base_level_predictions = np.column_stack((cnn_prediction, naive_bayes_prediction))\n",
        "\n",
        "cnbs = svm.SVC(kernel='linear')\n",
        "cnbs.fit(base_level_predictions, y_test)\n",
        "\n",
        "cnbs_prediction = cnbs.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(y_test, cnbs_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpIFqykVa2Ke",
        "outputId": "5820fc38-0a71-4a8e-b78d-0025bc84641f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6)CNN+ Naive bayes → KNN => CNBK\n",
        "cnbk = KNeighborsClassifier(n_neighbors=5)\n",
        "cnbk.fit(base_level_predictions, y_test)\n",
        "cnbk_prediction = cnbk.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(cnbk_prediction,y_test)\n",
        "print('Accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sacmu55ia5vC",
        "outputId": "e52ba59f-4e15-46a4-cdf2-d8b0fab763dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8202080237741456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) CNN + Naive bayes → Naive Bayes => CNBNB\n",
        "cnbnb = MultinomialNB()\n",
        "cnbnb.fit(base_level_predictions, y_test)\n",
        "cnbnb_prediction = cnbnb.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(cnbnb_prediction,y_test)\n",
        "print('Accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh2h942Ba88U",
        "outputId": "ffd26f19-6197-4f5b-a449-a60c330a8dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) CNN + Naive bayes → Decision tree => CNBD\n",
        "cnbd = DecisionTreeClassifier()\n",
        "cnbd.fit(base_level_predictions, y_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "cnbd_prediction = cnbd.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(y_test, cnbd_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOUm7Lqza_8V",
        "outputId": "dc03e327-6945-4fac-8c22-5dd1db27a53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9895988112927192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) CNN + Decision tree → SVM => CDS\n",
        "# Use base level predictions to train meta classifier\n",
        "base_level_predictions = np.column_stack((cnn_prediction, decision_tree_prediction))\n",
        "cds = svm.SVC(kernel='linear')\n",
        "cds.fit(base_level_predictions, y_test)\n",
        "\n",
        "cds_prediction = cds.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(y_test, cds_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03CuREhvbDfM",
        "outputId": "efd32f38-1392-498e-b5da-157bf4a0bb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) CNN + Decision tree → KNN => CDK\n",
        "cdk = KNeighborsClassifier(n_neighbors=5)\n",
        "cdk.fit(base_level_predictions, y_test)\n",
        "cdk_prediction = cdk.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(cdk_prediction,y_test)\n",
        "print('Accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cI9dhKTbGQm",
        "outputId": "7897872b-54ca-4c1a-e72e-dab518aeeda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8151931649331352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11) CNN + Decision tree → Naive Bayes => CDNB\n",
        "cdnb = MultinomialNB()\n",
        "cdnb.fit(base_level_predictions, y_test)\n",
        "cdnb_prediction = cdnb.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(cdnb_prediction,y_test)\n",
        "print('Accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BktuoukbIpD",
        "outputId": "64f4865f-f5bc-46e8-cd5b-805eb1e2bf75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12) CNN + Decision tree → Decision tree => CDD\n",
        "cdd = DecisionTreeClassifier()\n",
        "cdd.fit(base_level_predictions, y_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "cdd_prediction = cdd.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(y_test, cdd_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPmHlktmbLPg",
        "outputId": "ff08643b-3b39-4c05-a56f-3fbbd3b4317d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9856983655274889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13) CNN + SVM → SVM => CSS\n",
        "# Use base level predictions to train meta classifier\n",
        "base_level_predictions = np.column_stack((cnn_prediction, svm_predicatin))\n",
        "css = svm.SVC(kernel='linear')\n",
        "css.fit(base_level_predictions, y_test)\n",
        "\n",
        "css_prediction = css.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(y_test, css_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUewM8qObNzb",
        "outputId": "3a181ad0-8dc9-4615-cbcc-0a9db48cd053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14) CNN + SVM → KNN => CSK\n",
        "csk = KNeighborsClassifier(n_neighbors=5)\n",
        "csk.fit(base_level_predictions, y_test)\n",
        "csk_prediction = csk.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(csk_prediction,y_test)\n",
        "print('Accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTq2iCOQbROe",
        "outputId": "31f64cac-96f5-4132-b32d-679e9b3bc10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.812407132243685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15) CNN + SVM → Naive Bayes => CSNB\n",
        "csnb = MultinomialNB()\n",
        "csnb.fit(base_level_predictions, y_test)\n",
        "csnb_prediction = csnb.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(csnb_prediction,y_test)\n",
        "print('Accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5crPuxkfbTm7",
        "outputId": "628564dd-29c2-4706-ee63-c70d3592bce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7955052005943536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16) CNN + SVM → Decision tree => CSD\n",
        "csd = DecisionTreeClassifier()\n",
        "csd.fit(base_level_predictions, y_test)\n",
        "csd_prediction = csd.predict(base_level_predictions)\n",
        "accuracy = accuracy_score(y_test, csd_prediction)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBRWyXtrbWJ9",
        "outputId": "9b4d216b-6ea1-4e00-ffc6-4b54e7bd50c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9840267459138187\n"
          ]
        }
      ]
    }
  ]
}